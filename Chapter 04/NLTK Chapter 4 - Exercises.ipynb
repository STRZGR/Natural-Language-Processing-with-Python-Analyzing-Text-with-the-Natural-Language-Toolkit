{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Chapter 4\n",
    "\n",
    "## Writing Structured Programs\n",
    "\n",
    "*The html version of this chapter in the NLTK book is available [here](https://www.nltk.org/book/ch04.html#exercises \"Ch04 Exercises\").*\n",
    "\n",
    "### 4.11   Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###### 1. \n",
    "\n",
    "☼ Find out more about sequence objects using Python's help facility. In the interpreter, type `help(str)`, `help(list)`, and `help(tuple)`. This will give you a full list of the functions supported by each type. Some functions have special names flanked with underscore; as the help documentation shows, each such function corresponds to something more familiar. For example `x.__getitem__(y)` is just a long-winded way of saying `x[y]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. \n",
    "\n",
    "☼ Identify three operations that can be performed on both tuples and lists. Identify three list operations that cannot be performed on tuples. Name a context where using a list instead of a tuple generates a Python error.\n",
    "\n",
    "*Operations that can be performed on both tuples and lists:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in dir(list) if x in dir(tuple)], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Operations that can be performed on lists, but not tuples:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x for x in dir(list) if x not in dir(tuple)], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Trying to use a list as a key in a dictionary will not work, but it's possible with a tuple:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (\"Snugglebunnies\")\n",
    "b = [\"Basselopes\"]\n",
    "\n",
    "c = {a: \"N\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Saved as markdown because having a cell that throws an error will cause my notebook to go higgledy piddledy:*\n",
    "\n",
    "```\n",
    "c = {b: \"N\"}\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-17-ee5632d64820> in <module>\n",
    "----> 1 c = {b: \"N\"}\n",
    "\n",
    "TypeError: unhashable type: 'list'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is because tuples are hashable, but lists are not:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "hash(b)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-19-ad85d8b55702> in <module>\n",
    "----> 1 hash(b)\n",
    "\n",
    "TypeError: unhashable type: 'list'\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. \n",
    "\n",
    "☼ Find out how to create a tuple consisting of a single item. There are at least two ways to do this.\n",
    "\n",
    "*Add a common after a single value, or create a list with a single value, and use `tuple` to convert this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1, \n",
    "\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tuple([1])\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. \n",
    "\n",
    "☼ Create a list `words = ['is', 'NLP', 'fun', '?']`. Use a series of assignment statements (e.g. `words[1] = words[2]`) and a temporary variable `tmp` to transform this list into the list `['NLP', 'is', 'fun', '!']`. Now do the same transformation using tuple assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['is', 'NLP', 'fun', '?']\n",
    "tmp = words[0]\n",
    "words[0] = words[1]\n",
    "words[1] = tmp\n",
    "words[3] = '!'\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['is', 'NLP', 'fun', '?']\n",
    "words[1], words[0], words[3] = words[0], words[1], \"!\"\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.\n",
    "\n",
    "☼ Read about the built-in comparison function `cmp`, by typing `help(cmp)`. How does it differ in behavior from the comparison operators?\n",
    "\n",
    "*`cmp` has been deprecated.  Get with the times, authors!*\n",
    "\n",
    "```\n",
    "help(cmp)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "NameError                                 Traceback (most recent call last)\n",
    "<ipython-input-29-6cc5f65683db> in <module>\n",
    "----> 1 help(cmp)\n",
    "\n",
    "NameError: name 'cmp' is not defined\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. \n",
    "\n",
    "☼ Does the method for creating a sliding window of n-grams behave correctly for the two limiting cases: $n$ = 1, and $n$ = `len(sent)`?\n",
    "\n",
    "*No.  $n$ = 1 will just return __unigrams__, i.e., the individual words which comprised the sentence.  $n$ = `len(sent)` will just return the entire list:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "n = 1\n",
    "[sent[i:i+n] for i in range(len(sent)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    ">>> n = len(sent)\n",
    ">>> [sent[i:i+n] for i in range(len(sent)-n+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.\n",
    "\n",
    "☼ We pointed out that when empty strings and empty lists occur in the condition part of an `if` clause, they evaluate to `False`. In this case, they are said to be occurring in a Boolean context. Experiment with different kind of non-Boolean expressions in Boolean contexts, and see whether they evaluate as `True` or `False`.\n",
    "\n",
    "*With the exception of 0, all numbers evaluate as True.  Even `float('Inf')`, `-float('Inf')`, and `float('NaN')`. All strings evaluate as True, except for the empty string. Empty lists and tuples will evaluate as False.  `None` evaluates as false, but `None` in a list or a tuple evaluates as True.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cands = [0, 1, -1, float('Inf'), -float('Inf'), float('NaN'), \"0\", \"1\", \"\", [], \n",
    "         \"Fahrvergnügen\", 3.1415, None, [None], tuple([]), tuple([None])]\n",
    "\n",
    "for c in cands:\n",
    "    if c:\n",
    "        print(\"{} evaluates as True\".format(c))\n",
    "    else:\n",
    "        print(\"{} evaluates as False\".format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. \n",
    "\n",
    "☼ Use the inequality operators to compare strings, e.g. `'Monty' < 'Python'`. What happens when you do `'Z' < 'a'`? Try pairs of strings which have a common prefix, e.g. `'Monty' < 'Montague'`. Read up on \"lexicographical sort\" in order to understand what is going on here. Try comparing structured objects, e.g. `('Monty', 1) < ('Monty', 2)`. Does this behave as expected?\n",
    "\n",
    "*In lexicographical sort, only the first index in both items is compared.  Since 'M' comes before 'P', the rest of the string is ignored.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'M' < 'Python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'P'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Uppercase letters are considered as coming 'before' lowercase ones.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Z' < 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*'Monty' and 'Montague' have the same first four elements.  Since 'y' comes after 'a', the comparison evaluates as `False`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Monty' < 'Montague'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As the first elements in both tuples are identical, the second element is compared, and one is indeed less than 2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Monty', 1) < ('Monty', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Carrying on that logic a bit further:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Monty', 1, 1, 1, 5) < ('Monty', 1, 1, 1, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.\n",
    "\n",
    "☼ Write code that removes whitespace at the beginning and end of a string, and normalizes whitespace between words to be a single space character.\n",
    "\n",
    "* 1. do this task using `split()` and `join()`\n",
    "* 2. do this task using regular expression substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"    this    is   a really inconsistent          use  of     whitespace.      \"\n",
    "sent = sent.split()\n",
    "' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"    this    is   a really inconsistent          use  of     whitespace.      \"\n",
    "sent = re.sub(r'^\\s*|\\s*$', '', sent)\n",
    "sent = re.sub(r'\\s+', ' ', sent)\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10.\n",
    "\n",
    "☼ Write a program to sort words by length. Define a helper function `cmp_len` which uses the `cmp` comparison function on word lengths.\n",
    "\n",
    "*`cmp` is still deprecated, so I'll only do the first part of this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_length(text):\n",
    "    \"\"\"\n",
    "    Returns a list, sorted from shortest to longest,\n",
    "    of words in text.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [w for _, w in sorted([(len(w), w) for w in text.split()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'the words in this sentence are mostly of unique character lengths'\n",
    "\n",
    "print(sort_words_by_length(sent), end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11.\n",
    "\n",
    "◑ Create a list of words and store it in a variable `sent1`. Now assign `sent2 = sent1`. Modify one of the items in `sent1` and verify that `sent2` has changed.\n",
    "\n",
    "* a. Now try the same exercise but instead assign `sent2 = sent1[:]`. Modify sent1 again and see what happens to `sent2`. Explain.\n",
    "* b. Now define `text1` to be a list of lists of strings (e.g. to represent a text consisting of multiple sentences. Now assign `text2 = text1[:]`, assign a new value to one of the words, e.g. `text1[1][1] = 'Monty'`. Check what this did to `text2`. Explain.\n",
    "* c. Load Python's `deepcopy()` function (i.e. `from copy import deepcopy`), consult its documentation, and test that it makes a fresh copy of any object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = sent1\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__a.__ Using `[:]` causes `sent1` to be shallow copied, so changes aren't replicated in `sent2`.  I.e., it's a reference to a copy of the list, and not the original list.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = sent1[:]\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__ Now the change is permeated.  It appears a shallow copy of a list of lists is just a copy of the references of the lists.  I.e., if one of the original lists is changed, the changed is reflected.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA = \"Mairzy doats and dozy doats and liddle lamzy divey\"\n",
    "sentB = \"A kiddley divey too, wouldn't you?\"\n",
    "\n",
    "text1 = [sentA.split(), sentB.split()]\n",
    "text2 = text1[:]\n",
    "\n",
    "text1[1][1] = 'Monty'\n",
    "\n",
    "print(text2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we explicitly make shallow copies of the lists in the list of lists, then the changes won't be replicated:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentA = \"Mairzy doats and dozy doats and liddle lamzy divey\"\n",
    "sentB = \"A kiddley divey too, wouldn't you?\"\n",
    "\n",
    "text1 = [sentA.split(), sentB.split()]\n",
    "text2 = [text1[0][:], text1[1][:]]\n",
    "\n",
    "text1[1][1] = 'Monty'\n",
    "\n",
    "print(text2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "sent = \"Mairzy doats and dozy doats and liddle lamzy divey \" \\\n",
    "        \"A kiddley divey too, wouldn't you?\"\n",
    "sent1 = sent.split()\n",
    "sent2 = deepcopy(sent1)\n",
    "sent1[0] = \"Mares\"\n",
    "print(sent2, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. \n",
    "\n",
    "◑ Initialize an $n$-by-$m$ list of lists of empty strings using list multiplication, e.g. `word_table = [[''] * n] * m`. What happens when you set one of its values, e.g. `word_table[1][2] = \"hello\"`? Explain why this happens. Now write an expression using `range()` to construct a list of lists, and show that it does not have this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 6, 7\n",
    "\n",
    "word_table = [[''] * n] * m\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table[1][2] = (\"hello\")\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Above we have a copy of a list containing sublists. When we make a change to one of the sublists, the copy is replicated in all of the lists which have been created by copying it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table = [['' for i in range(n)] for j in range(m)]\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_table[1][2] = (\"hello\")\n",
    "pprint.pprint(word_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this case, we're creating a brand new empty string for each of the iterations through $n$, and likewise through $m$.  As the strings are not copies of each other, changes in one are not reflected in the others.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13. \n",
    "\n",
    "◑ Write code to initialize a two-dimensional array of sets called `word_vowels` and process a list of words, adding each word to `word_vowels[l][v]` where `l` is the length of the word and `v` is the number of vowels it contains.\n",
    "\n",
    "*If we follow the instructions literally and create an array of sets, both sets will come back sorted, and it is very likely that the $n$th item in the first set will not be the same as the $n$th item in the second.  I.e., a long word without many vowels would be represented towards the end of the first set, but towards the beginning of the second (and vice versa):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length_word_number_vowels(text):\n",
    "    \"\"\"\n",
    "    Returns an array of two sets.  The first set comprises\n",
    "    the lengths of the words.  The second the number of vowels.\n",
    "    For the sake of simplicity, 'y' is not counted as a vowel.\n",
    "    Results are ordered from smallest to greatest.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vowels = [set(), set()]\n",
    "\n",
    "    for t in text:\n",
    "        word_vowels[0].add(len(t))\n",
    "        word_vowels[1].add(sum([1 for i in t if i.lower() in 'aeiou']))\n",
    "\n",
    "    return word_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\"supercalifragilisticexpialidocious\", \"eye\", \"Hawai'i\", \"draft\"]\n",
    "find_length_word_number_vowels(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Using lists instead of sets will obviate this problem:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_length_word_number_vowels_by_order_of_appearance(text):\n",
    "    \"\"\"\n",
    "    Returns an array of two lists.  The first set comprises\n",
    "    the lengths of the words.  The second the number of vowels.\n",
    "    For the sake of simplicity, 'y' is not counted as a vowel.\n",
    "    Results are ordered by order of appearance of the original\n",
    "    word.\n",
    "    \"\"\"\n",
    "    \n",
    "    word_vowels = [[], []]\n",
    "\n",
    "    for t in text:\n",
    "        word_vowels[0].append(len(t))\n",
    "        \n",
    "        # taking advantage of the fact that `True` evaluates to 1\n",
    "        word_vowels[1].append(sum([1 for i in t if i.lower() in 'aeiou']))\n",
    "\n",
    "    return word_vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_length_word_number_vowels_by_order_of_appearance(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. \n",
    "\n",
    "◑ Write a function `novel10(text)` that prints any word that appeared in the last 10% of a text that had not been encountered earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def novel10(text):\n",
    "    \"\"\"\n",
    "    Returns a set of words that appear for the first time\n",
    "    in the last 10% of a text.\n",
    "    \"\"\"\n",
    "\n",
    "    split = int(len(text) * .9)\n",
    "    first90, last10 = text[:split], text[split:]\n",
    "    novel90 = set(first90)\n",
    "\n",
    "    return set([i for i in last10 if i not in novel90])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Mary Shelley's Frankenstein\n",
    "url = 'http://www.gutenberg.org/cache/epub/42324/pg42324.txt'\n",
    "\n",
    "frank = request.urlopen(url).read().decode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frank = word_tokenize(frank)\n",
    "\n",
    "# used trial and error (and `index`) to find and remove\n",
    "# header and footer\n",
    "frank = frank[115:90732]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Converted results to a list so that I could use slicing to get the first 20 items:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(novel10(frank))[:20], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15.\n",
    "\n",
    "◑ Write a program that takes a sentence expressed as a single string, splits it and counts up the words. Get it to print out each word and the word's frequency, one per line, in alphabetical order.\n",
    "\n",
    "*Although it's not best practice stylistically, the instructions say to print everything, so I'm going to do that instead of returning a value:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def print_words_and_frequency(text):\n",
    "    \"\"\"\n",
    "    Counts the words in a text and prints out the\n",
    "    resulting table in alphabetical order.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenizer separates words from punctuation\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    words = [t.lower() for t in tokens if t.isalpha()]\n",
    "    \n",
    "    # get word counts from FreqDist\n",
    "    ordered = sorted(set([(w, v) for w, v in nltk.FreqDist(words).items()]))\n",
    "    \n",
    "    # get widths for pretty printing\n",
    "    # width of longest word\n",
    "    width = max([len(w) for w, _ in ordered]) + 2\n",
    "    # width of longest number\n",
    "    width_counts = max([len(str(v)) for _, v in ordered])\n",
    "    \n",
    "    # print everything\n",
    "    for w, v in ordered:\n",
    "        print(\"{}:{}{:>{}}\".format(w, ' ' * (width - len(w)), \n",
    "                                     v, width_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"If police police police police, who polices the police police? \" \\\n",
    "       \"Police police police police police police.\"\n",
    "\n",
    "print_words_and_frequency(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"How much wood would a woodchuck chuck if a woodchuck could chuck wood?\"\n",
    "print_words_and_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16.\n",
    "\n",
    "◑ Read up on Gematria, a method for assigning numbers to words, and for mapping between words having the same number to discover the hidden meaning of texts (cf. [here](http://en.wikipedia.org/wiki/Gematria \"Wikipedia entry\"), or [here](http://essenes.net/gemcal.htm \"Gematria\")).\n",
    "\n",
    "* a. Write a function `gematria()` that sums the numerical values of the letters of a word, according to the letter values in `letter_vals`:\n",
    "\n",
    "``` \t\n",
    ">>> letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
    "... 'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
    "... 'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
    "```\n",
    "\n",
    "* b. Process a corpus (e.g. `nltk.corpus.state_union`) and for each document, count how many of its words have the number 666.\n",
    "\n",
    "* c. Write a function `decode()` to process a text, randomly replacing words with their Gematria equivalents, in order to discover the \"hidden meaning\" of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_vals = {'a':1, 'b':2, 'c':3, 'd':4, 'e':5, 'f':80, 'g':3, 'h':8,\n",
    "       'i':10, 'j':10, 'k':20, 'l':30, 'm':40, 'n':50, 'o':70, 'p':80, 'q':100,\n",
    "       'r':200, 's':300, 't':400, 'u':6, 'v':6, 'w':800, 'x':60, 'y':10, 'z':7}\n",
    "\n",
    "def gematria(word, vals = letter_vals):\n",
    "    \"\"\"\n",
    "    Returns Gematria value of a word.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    word: Word to be converted.\n",
    "    vals: Dictionary of values for each letter.\n",
    "          Default is `letter_vals`.\n",
    "    \"\"\"\n",
    "    \n",
    "    return sum(vals[w.lower()] for w in word if w in vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__b.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "su = nltk.corpus.state_union\n",
    "\n",
    "# for pretty printing later\n",
    "width = max([len(u[5:-4]) for u in su.fileids()])\n",
    "\n",
    "devil_words = [sum([1 for w in su.words(u) if gematria(w) == 666]) for u in su.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, w in zip(su.fileids(), devil_words):\n",
    "    print(u[:4], u[5:-4] + \":\" +\n",
    "          \" \" * (width - len(u[5:-4])),  \"{:>2}\".format(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__c.__ Since strings are immutable, we need to convert whatever format the `text` is in to a list. From there, we have several ways to choose words at random.  I originally chose $n$% of the index values at random and replaced those words.  But this was very slow with large texts, as the interpreter would have to traverse the entire list of words to find the word to be replaced.  I settled on the method below, which is much quicker: The interpreter only goes through the list of words once, and returns a random number between 0.0 and 1.0 for each word.  If the random number is below a certain threshold, the word is replaced.  With this method, there is a decent chance that the percentage of words replaced will differ from the percentage we specify, since the chances of one word being replaced are independent of all other words (i.e., as long as the threshold is not 0% or 100%, there's a chance that none of the words will be replaced, and also a chance that all of the words will be replaced).  Also, punctuation will never be replaced, since these marks don't have values in the dictionary `letter_vals`, which we use in the `gematria` function.*\n",
    "\n",
    "*Also using a function I used in [Chapter 2](https://github.com/Sturzgefahr/Natural-Language-Processing-with-Python-Analyzing-Text-with-the-Natural-Language-Toolkit/tree/master/Chapter%2002 \"Chapter 2 Exercises\") - `join_punctuation` - to reattach punctuation that was separated during tokenizing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current\n",
    "\n",
    "def decode(text, n = 10):\n",
    "    \"\"\"\n",
    "    Returns a copy of the original text with n percent\n",
    "    of the words converted into its gematria form.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text: Text can be any form. Will be converted into a list.\n",
    "    n:    Percentage of the words to be converted.  Should be\n",
    "          an integer between 0 and 100.   \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # convert a cp of text into a list\n",
    "    if type(text) == str:\n",
    "        # use tokenize to separate punctuation from words\n",
    "        cp = word_tokenize(text)    \n",
    "    elif type(text) == list:\n",
    "        cp = text[:]    \n",
    "    else:\n",
    "        cp = list(text[:])\n",
    "        \n",
    "\n",
    "    # go through the words in the list,\n",
    "    # and return a random number;\n",
    "    # if the random number is less than the percentage\n",
    "    # replace the word with its gematria value\n",
    "    for i in range(len(cp)):\n",
    "        if random.random() <= n/100: \n",
    "            cp[i] = str(gematria(cp[i]))\n",
    "        \n",
    "    \n",
    "    # using join punctuation to rejoin punctuation separated during\n",
    "    # tokenizing\n",
    "    return ' '.join(join_punctuation(cp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(\"This is just a test to see if my code will work\", 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(su.words('2006-GWBush.txt'), 33)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(nltk.corpus.gutenberg.words('austen-emma.txt'), 30)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 17. \n",
    "\n",
    "◑ Write a function `shorten(text, n)` to process a text, omitting the $n$ most frequently occurring words of the text. How readable is it?\n",
    "\n",
    "*The texts are usually quite readable if we delete the most common words, as most of these common words will be stop words.  However, in news articles and novels some of the most common words will be the names of the principals, and without those it's impossible to know who's doing what.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "\n",
    "def join_punctuation(text, characters = [\"'\", '’', ')', ',', '.', ':', ';', '?', '!', ']', \"''\"]): \n",
    "    \"\"\"\n",
    "    Takes a list of strings and attaches punctuation to\n",
    "    the preceding string in the list.\n",
    "    \"\"\"\n",
    "    \n",
    "    text = iter(text)\n",
    "    current = next(text)\n",
    "\n",
    "    for nxt in text:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "            \n",
    "\n",
    "    yield current\n",
    "\n",
    "\n",
    "def shorten(text, n):\n",
    "       \n",
    "    # convert a cp of text into a list\n",
    "    if type(text) == str:\n",
    "        # use tokenize to separate punctuation from words\n",
    "        cp = word_tokenize(text)    \n",
    "    elif type(text) == list:\n",
    "        cp = text[:]    \n",
    "    else:\n",
    "        cp = list(text[:])\n",
    "        \n",
    "    # get a list of most common words, and strip away the counts\n",
    "    most_common = [w for w, _ in nltk.FreqDist(w for w in cp if w.isalpha()).most_common(n)]\n",
    "    \n",
    "    # replace most common words\n",
    "    for i in range(len(cp)):\n",
    "        if cp[i] in most_common:\n",
    "            cp[i] = ''\n",
    "    \n",
    "    # join list and normalize whitespace - \n",
    "    # otherwise there'll be gaps for the missing words\n",
    "    # also use join_punctuation to reattach punctuation \n",
    "    # separate during tokenizing\n",
    "    return re.sub(r'\\s+', ' ', ' '.join(join_punctuation(cp)))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(\"This is a test which is a rather short one\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(su.words('2006-GWBush.txt'), 50)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(nltk.corpus.gutenberg.words('austen-emma.txt'), 50)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we delete an additional 50 common words, we'll lose mentions of \"Woodhouse\", one of the principal characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shorten(nltk.corpus.gutenberg.words('austen-emma.txt'), 100)[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18.\n",
    "\n",
    "◑ Write code to print out an index for a lexicon, allowing someone to look up words according to their meanings (or pronunciations; whatever properties are contained in lexical entries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'm a little confused by this question, since the notion of lexicon indexing was not introduced in the book, nor is it such a common concept (i.e., googling it doesn't bring up that many hits).*\n",
    " \n",
    "*From the best that I can understand, the idea is that we make a list of all items in lexicon $X$ with quality $Y_1$, all items with $Y_2$, and so on until we get to all items with $Y_n$.*\n",
    "\n",
    "*If we used the CMU Pronouncing Dictionary for this, the first index would be for words with the phoneme `AA0`, the next for words with `AA1`, and so on until we get to the last phoneme, `Y` (cf. [here](http://www.speech.cs.cmu.edu/cgi-bin/cmudict?in=C+M+U+Dictionary \"CMU dictionary\") for more information).*\n",
    "\n",
    "*It's difficult to see how we could do this with anything other than a Python dictionary, which really haven't been covered in much detail.  A Python dictionary can create an index of all the phonemes of the 133,737 words in a matter of seconds.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = nltk.corpus.cmudict.entries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of entries in the dictionary\n",
    "\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The dictionary uses the method `setdefault`, whose syntax I find quite twisted.  Regardless, the method creates a new item for phonemes it hasn't yet seen, and adds values for items that already exist.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {}\n",
    "\n",
    "for word, pronunciation in entries:\n",
    "        for phoneme in pronunciation:\n",
    "            # create item if it doesn't exist\n",
    "            # if it does exist, add current word\n",
    "            d.setdefault(phoneme, []).append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There are 70 phonemes in this dictionary.  There are 39 basic phonemes, but the vowels have multiple forms, depending where the stress lies.*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([1 for x in d.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Now I'll create a function that will find all items in the lexicon that have some specified properties.  We need at least one property to match (that of `arg1`), but we can use an unlimited number of additional properties, thanks to the `*args` argument.  From there, we'll use set intersections to find only those items which have all the specified properties.</i>\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_entries(d, arg1, *args):\n",
    "        \"\"\"\n",
    "        Returns a set of words using the phonemes\n",
    "        in arg1 and args.\n",
    "        \n",
    "        Arguments:\n",
    "        \n",
    "        d:    A dictionary of items.\n",
    "        arg1: The first index to be match.\n",
    "        args: Optional 2nd - nth indices to be matched.\n",
    "        \"\"\"\n",
    "        \n",
    "        rest = set(d[arg1])\n",
    "        for a in args:\n",
    "            rest = rest.intersection(set(d[a]))\n",
    "        \n",
    "        return rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d, 'AE2', 'B', 'D', 'IH0', 'K')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I'd like to do a sanity check to make sure my code is working correctly.  Finding pronunciations for specific words in the CMU dictionary is not so straight-forward, so I've made a function for this.  I'll check just two words, since I still have to verify everything manually:*\"\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pronuncation(corpus, target):\n",
    "        \"\"\"\n",
    "        Return pronunciation for target in corpus.\n",
    "        \"\"\"\n",
    "            \n",
    "        return [pronunciation for word, pronunciation in corpus if word == target]\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pronuncation(entries, 'abstracted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_pronuncation(entries, 'bundesbank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It takes a little longer to make a comparable dictionary of WordNet entries, but it's still possible.*\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}\n",
    "    \n",
    "for ss in wn.all_synsets():\n",
    "    for word in ss.definition().split():\n",
    "        word = re.sub(r\"[`'();,]\", \"\", word)\n",
    "        d2.setdefault(word, []).append(ss.name())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"able\", \"person\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"able\", \"person\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"sound\", \"instrument\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"sound\", \"instrument\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_all_entries(d2, \"electricity\", \"device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sanity check:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(find_all_entries(d2, \"electricity\", \"device\")):\n",
    "    print(\"* \" + wn.synset(x).definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19.\n",
    "\n",
    "◑ Write a list comprehension that sorts a list of WordNet synsets for proximity to a given synset. For example, given the synsets `minke_whale.n.01`, `orca.n.01`, `novel.n.01`, and `tortoise.n.01`, sort them according to their `shortest_path_distance()` from `right_whale.n.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right = wn.synset('right_whale.n.01')\n",
    "orca = wn.synset('orca.n.01')\n",
    "minke = wn.synset('minke_whale.n.01')\n",
    "tortoise = wn.synset('tortoise.n.01')\n",
    "novel = wn.synset('novel.n.01')\n",
    "\n",
    "whales = [orca, minke, tortoise, novel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(set([(right.path_similarity(w), w) for w in whales]), reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note the alternate names for __minke__ and __orca__:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(minke, orca, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 20. \n",
    "\n",
    "◑ Write a function that takes a list of words (containing duplicates) and returns a list of words (with no duplicates) sorted by decreasing frequency. E.g. if the input list contained 10 instances of the word `table` and 9 instances of the word `chair`, then `table` would appear before `chair` in the output list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_list_by_frequency(t):\n",
    "    return sorted(set([(w) for w, _ in nltk.FreqDist(t).items()]), \n",
    "                  reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(9):\n",
    "    test.append(\"chair\")\n",
    "for i in range(10):\n",
    "    test.append(\"table\")\n",
    "\n",
    "print(test, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_list_by_frequency(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21.\n",
    "\n",
    "◑ Write a function that takes a text and a vocabulary as its arguments and returns the set of words that appear in the text but not in the vocabulary. Both arguments can be represented as lists of strings. Can you do this in a single line, using `set.difference()`?\n",
    "\n",
    "*The instructions explicitly say that we only need a single line and the arguments are lists of strings.  However, considering all the advice given in this chapter, I would strongly recommend programming \"defensively\" and adding checks to make sure that the arguments are indeed lists.*\n",
    "\n",
    "*Here's what the function would look like as one line:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_vocab_not_in_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Returns strings in text but not in vocab.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text:  A list of strings.\n",
    "    vocab: A list of strings.\n",
    "    \"\"\"\n",
    "    return set(text).difference(set(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we program defensively and try to envision every combination of `str`, `set`, or `list`, we'd end up with a much longer function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def return_vocab_not_in_text(text, vocab):\n",
    "    \"\"\"\n",
    "    Returns strings in text but not in vocab.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    text:  A list of strings, a set of strings, or a string.\n",
    "    vocab: A list of strings, a set of strings, or a string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # if text and vocab are some combination of set and list\n",
    "    if isinstance(text, set) and isinstance(vocab, set):\n",
    "        return text.difference(vocab)\n",
    "    elif isinstance(text, list) and isinstance(vocab, set):\n",
    "        return set(text).difference(vocab)\n",
    "    elif isinstance(text, set) and isinstance(vocab, list):\n",
    "        return text.difference(set(vocab))\n",
    "    \n",
    "    # if text is a str\n",
    "    if type(text) == str:\n",
    "        text = word_tokenize(text)\n",
    "    else:\n",
    "        assert isinstance(text, \n",
    "                          list), \"Argument `text` must be a list or a string\"\n",
    "    \n",
    "    # if vocab is a str\n",
    "    if type(vocab) == str:\n",
    "        vocab = word_tokenize(vocab)\n",
    "    else:\n",
    "        assert isinstance(vocab, \n",
    "                          list), \"Argument `vocab` must be a list or a string\"\n",
    "\n",
    "    return set(text).difference(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "deep_thought = (\"When I was a kid my favorite relative was Uncle Caveman. \" \n",
    "                \"After school we'd all go play in his cave, \"\n",
    "                \"and every once in a while he would eat one of us. \" \n",
    "                \"It wasn't until later that I found out that Uncle Caveman \"\n",
    "                \"was a bear.\")\n",
    "\n",
    "# convert deep_thought to a list\n",
    "\n",
    "dt_words = word_tokenize(deep_thought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab =  [\"'d\", 'was', '.', 'play', 'school', 'favorite', 'found',  \n",
    "          'kid', 'all', 'once', 'Caveman', 'Uncle', 'cave', \n",
    "          'while', 'relative', \"n't\", 'until', 'out', 'we', 'a', 'my', \n",
    "          'After', 'that', 'every', 'later', 'and', 'go', 'in', 'of', \n",
    "          'one', 'bear', 'When', 'would', 'eat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(dt_words, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(deep_thought, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_vocab_not_in_text(set(dt_words), vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22.\n",
    "\n",
    "◑ Import the `itemgetter()` function from the operator module in Python's standard library (i.e. `from operator import itemgetter`). Create a list `words` containing several words. Now try calling: `sorted(words, key=itemgetter(1))`, and `sorted(words, key=itemgetter(-1))`. Explain what `itemgetter()` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "words = ['my', 'list', 'of', 'several', 'words']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`itemgetter(n)` retrieves the item at index position `n`. In the below examples, the list `word` is sorted by the key at index `n`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(words, key = itemgetter(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If the above examples were not clear, the tuple below should be easier to follow:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [('A', 'Beta', 3), ('C', 'Alpha', 2), ('B', 'Gamma', 1)]\n",
    "sorted(test, key = itemgetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test, key = itemgetter(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(test, key = itemgetter(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. \n",
    "\n",
    "◑ Write a recursive function `lookup(trie, key)` that looks up a key in a trie, and returns the value it finds. Extend the function to return a word when it is uniquely determined by its prefix (e.g. `vanguard` is the only word that starts with `vang-`, so `lookup(trie, 'vang')` should return the same thing as `lookup(trie, 'vanguard'))`.\n",
    "\n",
    "*I tried to use recursion as much as possible when solving this question.  I was able to do this for the first part of the question - simple lookup of a word in the tree; but I had some problems when I tried to expand this so that we could find words solely by their prefix.  If a word was uniquely determined by its prefix - as per the example in the question - then I was able to return the value solely through recursion. However, for non-unique prefixes, there would be several potential matches, but this function would only return the first, without notifying the user that there were other possible matches. I found this of limited practical usage, since an end user would probably not be aware if a prefix was unique or not. I therefore expanded the function even more so that it output all possible final values.  To tackle this final problem I had to resort to a non-recursive method - namely, a second function that would handle the special case of non-unique prefixes.  As far as I am aware, it would not be possible to do this entirely recursively.*\n",
    "\n",
    "*The function `insert` from chapter 4 in the NLTK book:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert(trie, key, value):\n",
    "    if key:\n",
    "        first, rest = key[0], key[1:]\n",
    "        if first not in trie:\n",
    "            trie[first] = {}\n",
    "        insert(trie[first], rest, value)\n",
    "    else:\n",
    "        trie['value'] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie = {}\n",
    "\n",
    "en = [\"vandalism\", \"vandalize\", \"vane\", \"vanguard\", \"vanilla\", \"vanish\", \n",
    "      \"vanity\"]\n",
    "fr = [\"vandalisme\", \"vandaliser\", \"girouette\", \"avant-garde\", \"vanille\", \n",
    "      \"disparaître\", \"vanité\"]\n",
    "\n",
    "[insert(trie, e, f) for e, f in zip(en, fr)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(trie, width = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The original version of the function that I wrote.  It looks up a key and returns its value:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        return trie[word]     \n",
    "    first, rest = word[0], word[1:]\n",
    "    if first not in trie:\n",
    "        return False\n",
    "    return lookup(trie[first], rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test - word not in trie\n",
    "\n",
    "lookup(trie, 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The expanded version is able to handle portions of keys, as long as those portions are unique to a specific key.  Otherwise, the function merely returns the first value to match that portion of a key.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded version - returns the value for complete words and unique suffixes\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    Word can be a complete word of just a prefix, i.e., \n",
    "    beginning of the string.  Function will only return the \n",
    "    first match for non-unique prefixes.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)\n",
    "\n",
    "    first, rest = word[0], word[1:]\n",
    "    if first not in trie:\n",
    "        return False\n",
    "    return lookup(trie[first], rest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vanil')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The final version is able to handle non-unique portions of keys, but resorts to a helper function to do this.  Therefore, it is not entirely recursive:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semi-recursive version; able to handle non-unique prefixes\n",
    "\n",
    "def get_final_value(trie, word):\n",
    "    \"\"\"\n",
    "    Retrieves the final value of word\n",
    "    in a trie.\n",
    "    \"\"\"\n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "\n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)     \n",
    "    return results\n",
    "\n",
    "def lookup(trie, word):\n",
    "    \"\"\"\n",
    "    Looks up the value of a word in a trie.\n",
    "    Word can be a complete word of just a prefix, i.e., \n",
    "    beginning of the string.  Function will return all \n",
    "    possible matches for non-unique prefixes.\n",
    "    \"\"\"\n",
    "        \n",
    "    if len(word) == 1:\n",
    "        keys = list(trie[word].keys())\n",
    "        if len(keys) > 1:\n",
    "            results = []\n",
    "            for k in keys:\n",
    "                results.append(get_final_value(trie[word], k))\n",
    "            return results\n",
    "        \n",
    "        if keys[0] == 'value':\n",
    "            return trie[word].values()\n",
    "        else:\n",
    "            for k in keys:\n",
    "                return lookup(trie[word], k)\n",
    "            \n",
    "        \n",
    "    if len(word) > 1:\n",
    "        \n",
    "        first, rest = word[0], word[1:]\n",
    "        if first not in trie:\n",
    "            return False\n",
    "        return lookup(trie[first], rest)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vang')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup(trie, 'vand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24.\n",
    "\n",
    "◑ Read up on \"keyword linkage\" (chapter 5 of (Scott & Tribble, 2006)). Extract keywords from NLTK's Shakespeare Corpus and using the NetworkX package, plot keyword linkage networks.\n",
    "\n",
    "*This was another question that I found to be quite vexing. Outside of this question, there is no mention of 'keyword linkage' in the book.  Furthermore, the Shakespeare Corpus is in xml format, which isn't covered until the last chapter of the book.*\n",
    "\n",
    "*There is some example code about the Shakespeare Corpus [here](http://www.nltk.org/howto/corpus.html, \"howto/corpus\"), but in my opinion it's not terribly helpful.  I was only able to access the lines of the plays after I had located an [external blog](https://www.datasciencebytes.com/bytes/2014/12/30/topic-modeling-of-shakespeare-characters/ \"topic modeling in Shakespeare\") where someone discussed dealing with a similar problem.*\n",
    "\n",
    "*Furthermore, even after I was able to locate a copy of Scott & Tribble, I found it quite difficult to replicate their results.  I found their discussion of keywords to be very general, and it was difficult to get the exact keywords in \"Romeo & Juliet\" that they had found.*\n",
    "\n",
    "*Finally, I'm not convinced that NetworkX is the best module for this specific task.  The example in the book showed WordNet hierarchies, but the network in this example is much shallower.  The networks for this problem have a lot of nodes and only a few edges, which make for a very cluttered graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The xml text is stored in this format, making it difficult to grab text from collocational windows (i.e., 5 words before and after a keyword).*\n",
    "\n",
    "\n",
    "```{xml}\n",
    "<SPEECH>\n",
    "<SPEAKER>BENVOLIO</SPEAKER>\n",
    "<LINE>Why, Romeo, art thou mad?</LINE>\n",
    "</SPEECH>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import shakespeare\n",
    "from xml.etree import ElementTree\n",
    "shakespeare.fileids() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "personae = [persona.text for persona in\n",
    "             play.findall('PERSONAE/PERSONA')]\n",
    "print(personae) # doctest: +ELLIPSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = set(speaker.text for speaker in\n",
    "                play.findall('*/*/*/SPEAKER'))\n",
    "\n",
    "print(speakers, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Not quite what I had wanted, but a dictionary of all lines with that mention Romeo:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Romeo = {}\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "for act in play.findall('ACT'):\n",
    "    for scene in act.findall('SCENE'):\n",
    "        for speech in scene.findall('SPEECH'):\n",
    "            for line in speech.findall('LINE'):\n",
    "                if 'Romeo' in str(line.text):\n",
    "                    print(line.text)\n",
    "                    for word in word_tokenize(line.text):\n",
    "                        Romeo[word] = 1 + Romeo.get(word, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Romeo, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>[The blog](https://www.datasciencebytes.com/bytes/2014/12/30/topic-modeling-of-shakespeare-characters/ \"Topic modeling in Shakespeare\") that I referenced earlier had a method for making a dictionary where all of the characters in a play would be the keys, and all of their lines the values.  What follows below is modified code from what was found at the linked blog:<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "lines = defaultdict(list)\n",
    "linecounts = defaultdict(int)\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "for child in play.findall('ACT/SCENE/SPEECH'):\n",
    "    speaker = child.find('SPEAKER').text\n",
    "    for line in child.findall('LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    lines[speaker].append(word)\n",
    "                    linecounts[speaker] += 1\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a dictionary of all the words said by Romeo, followed by their counts:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romeo = {}\n",
    "\n",
    "for w in lines[\"ROMEO\"]:\n",
    "    romeo[w] = 1 + romeo.get(w, 0)\n",
    "    \n",
    "from operator import itemgetter\n",
    "\n",
    "print(sorted(romeo.items(), key = itemgetter(1), reverse = True)[:20], end = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting raw text from Romeo & Juliet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "raw_rj = []\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "    if line.text is not None:\n",
    "        for word in tokenizer.tokenize(line.text):\n",
    "            if word not in stopwords and len(word) > 2:\n",
    "                raw_rj.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_rj[:20], end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Getting raw text from other Shakespeare plays in this corpus:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_others = []\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "for p in shakespeare.fileids():\n",
    "    # tokenize all plays EXCEPT R & J\n",
    "    if p != 'r_and_j.xml':\n",
    "        play = shakespeare.xml(p)\n",
    "    \n",
    "    \n",
    "    for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    raw_others.append(word)\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*One way to think of keywords is all the words in one text that do not appear in comparable texts.  Here, `rj_kw` is the set difference between all the words in \"Romeo and Juliet\" and all the words in the other plays in the corpus.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rj_kw = set(raw_rj) - set(raw_others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Romeo\" in rj_kw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But we should get rid of __hapax legomenon__ (words that only occur once), as well as __dis legomenon__ (words that occur only twice):*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts in raw R&J text\n",
    "\n",
    "raw_rj_dict = {}\n",
    "\n",
    "for w in raw_rj:\n",
    "    raw_rj_dict[w] = 1 + raw_rj_dict.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dictionary\n",
    "\n",
    "potential_rj_keywords = sorted(raw_rj_dict.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the words that appear more than twice\n",
    "\n",
    "rj_keywords = [w for w, v in potential_rj_keywords if v > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rj_keywords[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The code below makes a master list of keywords in the plays other than R & J in our corpus.  In order to make it to the master list, a word has to appear at least twice in at least one play.  This code is basically a combination of other pieces of code that have been used up to now:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'*\\w+\")\n",
    "\n",
    "# master list of keywords in all plays\n",
    "raw_others_keywords = []\n",
    "\n",
    "for p in shakespeare.fileids():\n",
    "    # tokenize all plays EXCEPT R & J\n",
    "    if p != 'r_and_j.xml':\n",
    "        play = shakespeare.xml(p)\n",
    "    \n",
    "    # temporary file for all words in current play\n",
    "    temp_raw = []\n",
    "    \n",
    "    # append all words to temp file\n",
    "    for line in play.findall('ACT/SCENE/SPEECH/LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                if word not in stopwords and len(word) > 2:\n",
    "                    temp_raw.append(word)\n",
    "                    \n",
    "    # make temporary dictionary and tally words in current play\n",
    "    temp_d = {}\n",
    "    for w in temp_raw:\n",
    "        temp_d[w] = 1 + temp_d.get(w, 0)\n",
    "\n",
    "    # if word occurs more than twice, add to master list\n",
    "    for w, v in temp_d.items():\n",
    "        if v > 2:\n",
    "            raw_others_keywords.append(w)\n",
    "            \n",
    "# get rid of duplicates\n",
    "raw_others_keywords = set(raw_others_keywords)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# substract kws from all plays from kws in R & J\n",
    "rj_only_kw = set(rj_keywords) - raw_others_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by count\n",
    "\n",
    "potential_rj_keywords = sorted(raw_rj_dict.items(), key = itemgetter(1), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only those words that appear more than twice, and only in R & J\n",
    "\n",
    "rj_kw = sorted(set(w for w,v in potential_rj_keywords if w in rj_only_kw and v >= 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rj_kw, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is very close to the method described in Scott and Tribble, but my list of resulting keywords seems to be much larger.  It should be remember that Scott & Tribble used a much larger corpus of Shakespeare works.*\n",
    "\n",
    "*Next, I tried to make a dictionary for each of the main characters, with the most common words used by each:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "romeo = {}\n",
    "\n",
    "for w in lines[\"ROMEO\"]:\n",
    "    if w in rj_kw:\n",
    "        romeo[w] = 1 + romeo.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(romeo.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tybalt = {}\n",
    "\n",
    "for w in lines[\"TYBALT\"]:\n",
    "    if w in rj_kw:\n",
    "        tybalt[w] = 1 + tybalt.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tybalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "juliet = {}\n",
    "\n",
    "for w in lines[\"JULIET\"]:\n",
    "    if w in rj_kw:\n",
    "        juliet[w] = 1 + juliet.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "sorted(juliet.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse = {}\n",
    "\n",
    "for w in lines[\"NURSE\"]:\n",
    "    if w in rj_kw:\n",
    "        nurse[w] = 1 + nurse.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mercutio = {}\n",
    "\n",
    "for w in lines[\"MERCUTIO\"]:\n",
    "    if w in rj_kw:\n",
    "        mercutio[w] = 1 + mercutio.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(mercutio.items(), key = itemgetter(1), reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris = {}\n",
    "\n",
    "for w in lines[\"PARIS\"]:\n",
    "    if w in rj_kw:\n",
    "        paris[w] = 1 + paris.get(w, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I then took five of the main characters, and created dictionaries with the ten keywords most often used by each charcter.  I used these dictionaries to produce a network graph:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rom = [w for w, _ in sorted(romeo.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "jul = [w for w, _ in sorted(juliet.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "tyb = [w for w, _ in sorted(tybalt.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "mer = [w for w, _ in sorted(mercutio.items(), key = itemgetter(1), reverse = True)[:10]]\n",
    "par = [w for w, _ in sorted(paris.items(), key = itemgetter(1), reverse = True)[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Romeo': rom, 'Juliet': jul, 'Tybalt': tyb, 'Mercutio': mer, 'Paris': par}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as  nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = nx.DiGraph(d)\n",
    "\n",
    "g.add_nodes_from(d.keys())\n",
    "\n",
    "nx.draw(g, with_labels=True, node_size = 100,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spring layout\n",
    "g = nx.DiGraph(d)\n",
    "\n",
    "g.add_nodes_from(d.keys())\n",
    "\n",
    "pos = nx.spring_layout(g)\n",
    "\n",
    "nx.draw_networkx(g, pos, node_size = 100, edge_color='y', alpha=.8, linewidths=0, font_size = 12)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The resulting graph isn't very helpful.  Let's use the exact keywords given in Scott & Tribble.  To get these keywords, the authors made a list of keywords occurring within a collocation window of five words before or after the target words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMEO = [\"Benvolio\", \"Juliet\", \"banished\", \"night\", \"Tybalt\", \"art\", \"dead\", \n",
    "         \"O\", \"thou\", \"is\"]\n",
    "TYBALT = [\"Capulet\", \"dead\", \"kinsman\", \"art\", \"O\", \"Mercutio\", \"slain\", \"is\", \n",
    "          \"thou\", \"Romeo\"]\n",
    "JULIET =  [\"she\", \"O\", \"lady\", \"thou\", \"thee\", \"thy\", \"dead\", \"Romeo\", \"Nurse\"]\n",
    "CAPULET = [\"thee\", \"Friar\", \"Tybalt\", \"Juliet\", \"Montague\", \"is\", \"Paris\", \n",
    "           \"Capulet’s\", \"nurse\", \"Lady\"]\n",
    "NURSE = [\"thy\", \"she\", \"Peter\", \"thee\", \"is\", \"Juliet\", \"Capulet\", \"thou\", \n",
    "         \"Lady\", \"O\"]\n",
    "NIGHT = [\"light\", \"torch\", \"O\", \"she\", \"thy\", \"thee\", \"love\", \"Romeo\", \"thou\", \"is\"]\n",
    "MERCUTIO = [\"she\", \"O\", \"kinsman\", \"is\", \"thy\", \"thou\", \"lady\", \"Romeo\", \n",
    "            \"Tybalt\", \"Benvolio\"]\n",
    "PARIS = [\"Thursday\", \"married\", \"Lawrence\", \"Friar\", \"love\", \"is\", \"Romeo\", \n",
    "         \"dead\", \"Capulet\", \"County\"]\n",
    "LOVE = [\"Paris\", \"Lady\", \"death\", \"night\", \"she\", \"thee\", \"thou\", \"is\", \"O\", \n",
    "        \"thy\"]\n",
    "MONTAGUE = [\"thee\", \"Benvolio\", \"O\", \"thy\", \"art\", \"Lady\", \"Romeo\", \"thou\", \n",
    "            \"Capulet\", \"is\"]\n",
    "THOU = [\"death\", \"night\", \"O\", \"love\", \"is\", \"Romeo\", \"thee\", \"wilt\", \"thy\", \n",
    "        \"art\"]\n",
    "FRIAR = [\"Romeo\", \"Nurse\", \"Capulet\", \"Lady\", \"Mantua\", \"Paris\", \"O\", \"is\", \n",
    "         \"cell\", \"Lawrence\"]\n",
    "ROMEOS = [\"she\", \"dead\", \"banished\", \"Romeo\", \"thou\", \"Friar\", \"Tybalt’s\", \n",
    "          \"watch\", \"O\", \"is\"]\n",
    "O = [\"Juliet\", \"Friar\", \"she\", \"Nurse\", \"thee\", \"thy\", \"Romeo\", \"thou\", \"love\", \n",
    "     \"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'Romeo': ROMEO, 'Tybalt': TYBALT, 'Juliet': JULIET, 'Capulet': CAPULET, 'n': NURSE, 'ni': NIGHT, \n",
    "      'm': MERCUTIO, 'p': PARIS, 'l': LOVE, 'mo': MONTAGUE, 'th': THOU, 'f': FRIAR, 'rs': ROMEOS, 'o': O}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = nx.DiGraph(d2)\n",
    "\n",
    "g2.add_nodes_from(d2.keys())\n",
    "\n",
    "\n",
    "for k, v in d2.items():\n",
    "    g2.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g2, with_labels=True, node_size = 100,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = nx.DiGraph(d2)\n",
    "\n",
    "g2.add_nodes_from(d2.keys())\n",
    "\n",
    "\n",
    "for k, v in d2.items():\n",
    "    g2.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g2, with_labels=True)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This graph is much too busy.  Let's try a smaller network, with just the two principals and the main characters from the house of Montague:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = {'Romeo': ROMEO,  'Juliet': JULIET,  'Mercutio': MERCUTIO,  'Montague': MONTAGUE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = nx.DiGraph(d3)\n",
    "\n",
    "g3.add_nodes_from(d3.keys())\n",
    "\n",
    "\n",
    "for k, v in d3.items():\n",
    "    g3.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g3, with_labels=True, node_size = 10,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try the same with the house of Capulet:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = {'Romeo': ROMEO,  'Juliet': JULIET,  'Capulet': CAPULET, 'Nurse': NURSE, 'Paris': PARIS,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g4 = nx.DiGraph(d4)\n",
    "\n",
    "g4.add_nodes_from(d4.keys())\n",
    "\n",
    "\n",
    "for k, v in d4.items():\n",
    "    g4.add_edges_from(([(k, t) for t in v]))\n",
    "\n",
    "nx.draw(g4, with_labels=True, node_size = 10,font_size=12)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Below was code I used to try to replicate the keyword selection in Scott & Tribble.  The code below find words within a collocation window of five words before and after a mention of 'Romeo'. The XML file lists each line separately, and if a target word began a line, it would be difficult to grab the last five words from the previous line.  To simplify things, I stripped all the lines of text from the XML file and concatenated it to one long list.  To prevent the words of two speakers from appearing in two collocation windows, I added five empty strings before the beginning of every speaker's line.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+'?\\w+\")\n",
    "\n",
    "play = shakespeare.xml('r_and_j.xml')\n",
    "\n",
    "raw = []\n",
    "\n",
    "for child in play.findall('ACT/SCENE/SPEECH'):\n",
    "    speaker = child.find('SPEAKER').text\n",
    "    for i in range(5):\n",
    "        raw.append(\"\")\n",
    "    for line in child.findall('LINE'):\n",
    "        if line.text is not None:\n",
    "            for word in tokenizer.tokenize(line.text):\n",
    "                raw.append(word)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw[:150], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words from collocation windows with 'Romeo'\n",
    "romeo_ky = [raw[i - 5 :i] + raw[i + 1 : i + 6] for i in range(len(raw)) if raw[i] == \"Romeo\"]\n",
    "\n",
    "# only keep those words in R&J keywords\n",
    "romeo_ky = [w for sl in romeo_ky for w in sl if w in rj_kw]\n",
    "\n",
    "# get counts\n",
    "romeo_kyd = {}\n",
    "for w in romeo_ky:\n",
    "    romeo_kyd[w] = 1 + romeo_kyd.get(w, 0)\n",
    "    \n",
    "# sort by count\n",
    "from operator import itemgetter\n",
    "   \n",
    "romeo_ky = [w for w, _ in sorted(romeo_kyd.items(), \n",
    "                                 key = itemgetter(1), reverse = True)]\n",
    "print(romeo_ky, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare that to the keywords listed in Scott & Tribble:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROMEO = [\"Benvolio\", \"Juliet\", \"banished\", \"night\", \"Tybalt\", \"art\", \"dead\", \n",
    "         \"O\", \"thou\", \"is\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As I mentioned earlier, instead of giving a step-by-step breakdown of their process for determining keywords, Scott & Tribble instead presented it more as a narrative, making it quite difficult to replicate their results.*\n",
    "\n",
    "*When examining our - fairly disappointing - final graphs, something we have to remember is that Scott & Tribble never discussed actual linked keyword networks; rather hypothetical ones:*\n",
    "\n",
    "* \"We may now hypothesise that a KW plot such as the ones we saw in the previous chapter could be redrawn as a network of connections consisting of configurations reminiscent of strings, stars, cliques and clumps of which the next figure shows a fragment.\" (Scott & Tribble, p. 75)\n",
    "\n",
    "*From the graphs above, we can see that the actual graphs would be far messier than hypothesized.  The number of overlapping edges is large and makes the graph very difficult to read.  The only way to remedy this is to use a select subset of keywords for a selection of characters.  But the process of selecting keywords is quite laborious and might outweigh any benefits of seeing these networks visually.*\n",
    "\n",
    "*I would argue that this approach is not as practical as Scott & Tribble hypothesized.  While one may say that my personal inexperience with NetworkX is responsible for the less-than-ideal graphs above, a Google image search for \"linguistics keyword linkage graph\" failed to produce any relevant results. This seems to indicate that in the 13 years since \"Textual Patterns\" was published, no one has been able to actualize and publish any linked network graphs in the vein of what Scott & Tribble proposed, and this approach is not a viable one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. \n",
    "\n",
    "◑ Read about string edit distance and the Levenshtein Algorithm. Try the implementation provided in `nltk.edit_distance()`. In what way is this using dynamic programming? Does it use the bottom-up or top-down approach? [See also http://norvig.com/spell-correct.html]\n",
    "\n",
    "*The implementation uses a bottom-up approach: It breaks the problem into sub-problems, and solves these one by one until the edit distance has been found.  Since many of the sub-problems will have overlapping solutions, this implementation saves time by using dynamic programming, as the function merely retrieves the solution found earlier, instead of calculating it anew.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.edit_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('water', 'wine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('something', 'seething')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('cat', 'dog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.edit_distance('boy', 'girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. \n",
    "\n",
    "◑ The Catalan numbers arise in many applications of combinatorial mathematics, including the counting of parse trees. The series can be defined as follows: $C_0 = 1$, and $C_{n+1} = \\sum_{0..n} (C_iC_{n-i})$.\n",
    "\n",
    "* a. Write a recursive function to compute $n$th Catalan number $C_n$.\n",
    "\n",
    "* b. Now write another function that does this computation using dynamic programming.\n",
    "\n",
    "* c. Use the timeit module to compare the performance of these functions as n increases.\n",
    "\n",
    "*After a little research, I discovered that the only way to get this to work as a recursive program is to rewrite the formula as $C_n = \\sum_0^n (C_iC_{n-1-i})$:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Catalan(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Catalan_memo(n, lookup = {0: 1}):\n",
    "    if n not in lookup:\n",
    "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
    "        lookup[n] = result\n",
    "    return lookup[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan_memo(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import memoize\n",
    "@memoize\n",
    "\n",
    "def Catalan_memoize(n):\n",
    "    if n < 1:\n",
    "        return 1\n",
    "    else:\n",
    "        result = sum([(Catalan(i) * Catalan(n - 1 - i)) for i in range(n)])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Catalan_memoize(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We can use `%%time` to time operations in jupyter notebooks:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan_memo(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print([Catalan_memoize(i) for i in range(15)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This [website](https://rosettacode.org/wiki/Catalan_numbers#Python \"printing the results for multiple functions at once\") had some nice code for running several functions at once and printing the results in a presentable format.  Below is the code I found at the site, with slight modifications:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(defs, results):\n",
    "    fmt = '%-16s ' * 3\n",
    "    print((fmt % tuple(c.__name__ for c in defs)).upper())\n",
    "    print(fmt % ((\"=\" * 16,) * 3))\n",
    "    for r in zip(*results):\n",
    "        print(fmt % r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defs = (Catalan, Catalan_memo, Catalan_memoize)\n",
    "results = [tuple(c(i) for i in range(15)) for c in defs]\n",
    "pr(defs, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. \n",
    "\n",
    "★ Reproduce some of the results of (Zhao & Zobel, 2007) concerning authorship identification.\n",
    "\n",
    "*Zhao & Zobel is easily found online.  [Here](https://www.researchgate.net/publication/221574042_Searching_With_Style_Authorship_Attribution_in_Classic_Literature \"Zhao & Zobel\") is the site where I found my copy.*\n",
    "\n",
    "*Their study would be very difficult to replicate, especially for someone only on chapter 4 of the NLTK.  The study dealt with a huge number of texts (634 Project Gutenberg texts and 250,000 newswire articles), and some of the methods used are either NLP methods that have not been covered yet in this book (e.g., part-of-speech tagging), or fairly complicated statistical techniques that are beyond the purview of the book, i.e., the Kullback-Leibler divergence.*\n",
    "\n",
    "*Since I didn't want to reinvent the wheel for such a complicated experiment, I did a little googling to see if someone else had undertaken a comparable experiment.  I found one [here](http://www.aicbt.com/authorship-attribution/ \"Authorship Attribution\") that looked promising.  The author in this case use machine learning techniques to examine one book with two authors, and tried to determine which author had written which chapter.*\n",
    "\n",
    "*I aimed to analyze the same authors as in Zhao & Zobel - namely Shakespeare (WS) and Christopher Marlowe (CM) - and used six plays from Marlowe and seven from Shakespeare.  Since Marlowe was a tragedian, I used only tragedies for Shakespeare's works.  The thirteen works were \"Antony and Cleopatra\" (WS); \"Coriolanus\" (WS); \"Dido\" (CM); \"Dr. Faustus\" (CM); \"Edward II\" (CM); \"Hamlet\" (WS); \"The Jew of Malta\" (CW); \"Julius Caesar\" (WS); \"King Lear\" (WS); \"MacBeth\" (WS); \"Othello\" (WS); \"Tamburlaine pt. I\" (CM); and \"Tamburlaine pt. II\" (CM).*\n",
    "\n",
    "*Most of the code which follows is based on the code at that blog, with slight modifications for this task. Namely, the original code considered 12 chapters of a single book, whereas I considered 13 plays.  Also, the original code used punctuation as a method for attributing authors, specifically semicolons and colons.  However, we don't have the original texts of the majority (if not the entirety) of plays from Elizabethan era playwrights, so any differences in punctuation may be due to the editors of the various editions of these plays.  I.e., the differences in the use of semicolons/colons are probably not a good way to identify 16th/17th-century authors.  Nonetheless, I was curious if the number of exclamation marks could indicate who wrote which play, so I left this code in.*\n",
    "\n",
    "*All the texts examined were downloaded from Project Gutenberg (PG).  In the case where there were several editions of the same text, I chose the one with the most downloads.  Although I have written code to programmatically remove header and footer information from PG texts, for this experiment I decided to inspect each text manually: I was looking for editorial annotations that might cause the classifier to favor one author over the other, and deleted non-textual markers such as brackets used to designate footnotes and underscores used to designate the names of speakers before their spoken lines.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, glob, os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts_Anon\"\n",
    "files = sorted(glob.glob(os.path.join(path, \"Text*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_text = ' '.join(texts)\n",
    "\n",
    "# get rid of carriage returns and other encoding remnants\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_text = re.sub(k, v, all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LexicalFeatures():\n",
    "    num_texts = len(texts)\n",
    "    fvs_lexical = np.zeros((num_texts, 3), np.float64)\n",
    "    fvs_punct = np.zeros((num_texts, 3), np.float64)\n",
    "\n",
    "    for e, n_text in enumerate(texts):\n",
    "        tokens = nltk.word_tokenize(n_text.lower())\n",
    "        words = word_tokenizer.tokenize(n_text.lower())\n",
    "        sentences = sentence_tokenizer.tokenize(n_text)\n",
    "\n",
    "        vocab = set(words)\n",
    "\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s)) \n",
    "                                       for s in sentences])\n",
    "\n",
    "        # average number of words per sentence\n",
    "        fvs_lexical[e, 0] = words_per_sentence.mean()\n",
    "        # sentence length variation\n",
    "        fvs_lexical[e, 1] = words_per_sentence.std()\n",
    "        # Lexical diversity\n",
    "        fvs_lexical[e, 2] = len(vocab) / float(len(words))\n",
    "\n",
    "        # Commas per sentence\n",
    "        fvs_punct[e, 0] = tokens.count(',') / float(len(sentences))\n",
    "        # Semicolons per sentence\n",
    "        fvs_punct[e, 1] = tokens.count(';') / float(len(sentences))\n",
    "        # Exclamation marks per sentence\n",
    "        fvs_punct[e, 2] = tokens.count('!') / float(len(sentences))\n",
    "        \n",
    "    \n",
    "\n",
    "    # apply whitening to decorrelate the features\n",
    "    fvs_lexical = whiten(fvs_lexical)\n",
    "    fvs_punct = whiten(fvs_punct)\n",
    "    \n",
    "    return fvs_lexical, fvs_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BagOfWords(n = 15):\n",
    "    # get most common words in the whole book\n",
    "    all_tokens = nltk.word_tokenize(all_text)\n",
    "    fdist = nltk.FreqDist(all_tokens)\n",
    "    vocab = [k for k,v in fdist.most_common(n)]\n",
    "\n",
    "    # use sklearn to create the bag of words feature vector for each text\n",
    "    vectorizer = CountVectorizer(vocabulary = vocab, tokenizer = nltk.word_tokenize)\n",
    "    fvs_bow = vectorizer.fit_transform(texts).toarray().astype(np.float64)\n",
    "\n",
    "    # normalize by dividing each row by its Euclidean norm\n",
    "    fvs_bow /= np.c_[np.apply_along_axis(np.linalg.norm, 1, fvs_bow)]\n",
    "    \n",
    "    return fvs_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SyntacticFeatures():\n",
    "    # get part of speech for each token in each chapter\n",
    "\n",
    "    def token_to_pos(ch):\n",
    "        tokens = nltk.word_tokenize(ch)\n",
    "        return [p[1] for p in nltk.pos_tag(tokens)]\n",
    "\n",
    "    text_pos = [token_to_pos(t) for t in texts]\n",
    "\n",
    "    # count frequencies for common POS types\n",
    "    pos_list = ['NN', 'NNP', 'DT', 'IN', 'JJ', 'NNS']\n",
    "    fvs_syntax = np.array([[t.count(pos) for pos in pos_list]\n",
    "                           for t in text_pos]).astype(np.float64)\n",
    "\n",
    "    # normalize by dividing each row by number of tokens in the chapter\n",
    "    fvs_syntax /= np.c_[np.array([len(t) for t in text_pos])]\n",
    "    \n",
    "    return fvs_syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAuthors(fvs):\n",
    "    km = KMeans(n_clusters = 2, init = 'k-means++', n_init = 10, verbose = 0)\n",
    "    km.fit(fvs)\n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = list(LexicalFeatures())\n",
    "feature_sets.append(BagOfWords(15))\n",
    "feature_sets.append(SyntacticFeatures())\n",
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Second attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Third attempt:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [PredictAuthors(fvs).labels_ for fvs in feature_sets]\n",
    "\n",
    "correct_answers = '1100010111100'\n",
    "\n",
    "print('Correct Answers:')\n",
    "\n",
    "print(' '.join(c for c in correct_answers))\n",
    "\n",
    "print('Predictions:')\n",
    "\n",
    "for results in classifications:\n",
    "    match = sum([1 for c, a in zip(correct_answers, results) if c == str(a)])\n",
    "    print(' '.join([str(a) for a in results]), end = ' ')\n",
    "    print(\"\\tCorrect Predictions:\", match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Unfortunately, the results are far from conclusive, and a bit difficult to decipher.  The four rows refer to __Lexical differences__; __Punctuation differences__; __Bag of Words__; and __Syntactic differences__.  Since there is no guide telling us which author is '0' and which is '1', we have to deduce which is which. (Usually, Marlowe is '0' and Shakespeare '1').  The results are usually middling, and making matters worse is the fact the model is rather inconsistent: the results tend to vary widely each time the experiment is run.  Part of this is probably due to the fact that the model uses K-means clustering, which to a large extent is based on random initial values.  However, since the results seem to be so different each time, it seems that the features are quite difficult to distinguish from each other, and that a better set of features needs to be used.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I decided to change tack and tried to replicate some of the findings from Zhao and Zobel.  Although my sample was much smaller, the initial figures I produced were quite different.  I focused on function words (closed set words such as articles, modal verbs, prepositions, etc...) and part-of-speech tags. For this experiment, I concatenated all the Shakespeare tragedies into one long string, and did the same for the Marlowe works:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Shakespeare data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "sh_files = sorted(glob.glob(os.path.join(path, \"Sh*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in sh_files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_sh = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_sh = re.sub(k, v, all_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Marlowe data\n",
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ma_files = sorted(glob.glob(os.path.join(path, \"Ma*.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ma_files:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "all_ma = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    all_ma = re.sub(k, v, all_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*List of function words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://semanticsimilarity.files.wordpress.com/2013/08/jim-oshea-fwlist-277.pdf\n",
    "\n",
    "function_words = [\"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \n",
    "                  \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \n",
    "                  \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \n",
    "                  \"amongst\", \"amoungst\", \"an\", \"and\", \"another\", \"any\", \n",
    "                  \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \n",
    "                  \"are\", \"around\", \"as\", \"at\", \"be\", \"became\", \"because\", \n",
    "                  \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\",\n",
    "                  \"beside\", \"besides\", \"between\", \"beyond\", \"both\", \"but\", \n",
    "                  \"by\", \"can\", \"cannot\", \"could\", \"dare\", \"despite\", \"did\", \n",
    "                  \"do\", \"does\", \"done\", \"down\", \"during\", \"each\", \"eg\", \n",
    "                  \"either\", \"else\", \"elsewhere\", \"enough\", \"etc\", \"even\", \n",
    "                  \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \n",
    "                  \"except\", \"few\", \"first\", \"for\", \"former\", \"formerly\", \n",
    "                  \"from\", \"further\", \"furthermore\", \"had\", \"has\", \"have\", \n",
    "                  \"he\", \"hence\", \"her\", \"here\", \"hereabouts\", \"hereafter\", \n",
    "                  \"hereby\", \"herein\", \"hereinafter\", \"heretofore\", \n",
    "                  \"hereunder\", \"hereupon\", \"herewith\", \"hers\", \"herself\", \n",
    "                  \"him\", \"himself\", \"his\", \"how\", \"however\", \"i\", \"ie\", \n",
    "                  \"if\", \"in\", \"indeed\", \"inside\", \"instead\", \"into\", \"is\", \n",
    "                  \"it\", \"its\", \"itself\", \"last\", \"latter\", \"latterly\", \n",
    "                  \"least\", \"less\", \"lot\", \"lots\", \"many\", \"may\", \"me\", \n",
    "                  \"meanwhile\", \"might\", \"mine\", \"more\", \"moreover\", \"most\", \n",
    "                  \"mostly\", \"much\", \"must\", \"my\", \"myself\", \"namely\", \n",
    "                  \"near\", \"need\", \"neither\", \"never\", \"nevertheless\", \n",
    "                  \"next\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \n",
    "                  \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \n",
    "                  \"oftentimes\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \n",
    "                  \"other\", \"others\", \"otherwise\", \"ought\", \"our\", \"ours\", \n",
    "                  \"ourselves\", \"out\", \"outside\", \"over\", \"per\", \"perhaps\", \n",
    "                  \"rather\", \"re\", \"same\", \"second\", \"several\", \"shall\", \n",
    "                  \"she\", \"should\", \"since\", \"so\", \"some\", \"somehow\", \n",
    "                  \"someone\", \"something\", \"sometime\", \"sometimes\", \n",
    "                  \"somewhat\", \"somewhere\", \"still\", \"such\", \"than\", \"that\", \n",
    "                  \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \n",
    "                  \"thence\", \"there\", \"thereabouts\", \"thereafter\", \"thereby\",\n",
    "                  \"therefore\", \"therein\", \"thereof\", \"thereon\", \"thereupon\", \n",
    "                  \"these\", \"they\", \"third\", \"this\", \"those\", \"though\", \n",
    "                  \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\",\n",
    "                  \"too\", \"top\", \"toward\", \"towards\", \"under\", \"until\", \"up\", \n",
    "                  \"upon\", \"us\", \"used\", \"very\", \"via\", \"was\", \"we\", \"well\", \n",
    "                  \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \n",
    "                  \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \n",
    "                  \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \n",
    "                  \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \n",
    "                  \"why\", \"whyever\", \"will\", \"with\", \"within\", \"without\", \n",
    "                  \"would\", \"yes\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \n",
    "                  \"yourselves\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Proportion of 'the', 'of', and 'a' amongst the function words used in Shakespeare:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_words = word_tokenizer.tokenize(all_sh.lower())\n",
    "sh_function = [w for w in sh_words if w in function_words]\n",
    "sh_dict = {}\n",
    "\n",
    "f_words = ['the', 'of', 'a']\n",
    "\n",
    "for f in f_words:\n",
    "    for w in sh_function:\n",
    "        if w == f:\n",
    "            sh_dict[f] = 1 + sh_dict.get(f, 0)\n",
    "            \n",
    "for f in f_words:\n",
    "    sh_dict[f] = (sh_dict[f]/len(sh_function)) * 100\n",
    "    \n",
    "sh_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is slightly different from the results found in Zhao & Zobel, which were respectively 7.6, 4.8, and 4.1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_words = word_tokenizer.tokenize(all_ma.lower())\n",
    "ma_function = [w for w in ma_words if w in function_words]\n",
    "ma_dict = {}\n",
    "\n",
    "f_words = ['the', 'of', 'a']\n",
    "\n",
    "for f in f_words:\n",
    "    for w in ma_function:\n",
    "        if w == f:\n",
    "            ma_dict[f] = 1 + ma_dict.get(f, 0)\n",
    "            \n",
    "for f in f_words:\n",
    "    ma_dict[f] = (ma_dict[f]/len(ma_function)) * 100\n",
    "    \n",
    "ma_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The respective results from Zhao & Zobel: 9.5, 6.2, 3.2.*\n",
    "\n",
    "*I then tried to establish what percentage of POS tags were coordinating conjunctions (CC), prepositions (IN), and adjectives (JJ).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slow to run\n",
    "\n",
    "sh_pos = [p[1] for p in nltk.pos_tag(all_sh)]\n",
    "ma_pos = [p[1] for p in nltk.pos_tag(all_ma)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_d = {}\n",
    "\n",
    "tags = ['CC', 'IN', 'JJ']\n",
    "\n",
    "for t in tags:\n",
    "    for p in sh_pos:\n",
    "        if p == t:\n",
    "            sh_pos_d[t] = 1 + sh_pos_d.get(t, 0)\n",
    "            \n",
    "for t in tags:\n",
    "    sh_pos_d[t] = (sh_pos_d[t]/len(sh_pos)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The results were quite different in Zhao & Zobel.  Respectively, they were 3.8, 5.9, and 2.8*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_d = {}\n",
    "\n",
    "tags = ['CC', 'IN', 'JJ']\n",
    "\n",
    "for t in tags:\n",
    "    for p in ma_pos:\n",
    "        if p == t:\n",
    "            ma_pos_d[t] = 1 + ma_pos_d.get(t, 0)\n",
    "            \n",
    "for t in tags:\n",
    "    ma_pos_d[t] = (ma_pos_d[t]/len(ma_pos)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again the respective results in Zhao & Zobel were 3.2, 6.4, and 2.4.  Obviously, a much different POS tagger was used.*\n",
    "\n",
    "*I was curious to know what would happen if I used Spearman's Correlation Coefficient on rankings of POS tags.  So I used a frequency distribution to create a ranking of the most commonly used POS tags for the Shakespeare and Marlowe corpora, and then calculated the correlation coefficient on the first test text: Shakespeare's \"Othello\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tags = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', \n",
    "            'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', \n",
    "            'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', '', 'UH', 'VB', \n",
    "            'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_pos_fd = nltk.FreqDist(sh_pos)\n",
    "sh_tags_sorted = [l for r, l in sorted([(v, k) for k, v in sh_pos_fd.items()], reverse = True)]\n",
    "print(sh_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_pos_fd = nltk.FreqDist(ma_pos)\n",
    "ma_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ma_pos_fd.items()], reverse = True)]\n",
    "print(ma_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho_file = sorted(glob.glob(os.path.join(path, \"Holdout1.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho = re.sub(k, v, ho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_pos = [p[1] for p in nltk.pos_tag(ho)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_pos_fd = nltk.FreqDist(ho_pos)\n",
    "ho_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho_pos_fd.items()], reverse = True)]\n",
    "print(ho_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.spearman import *\n",
    "\n",
    "sh_sc = spearman_correlation(ranks_from_sequence(ho_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The two coefficients are very similar, but the coefficient for the Shakespeare corpus is higher, indicating the correct author.  I tried a similar experiment, with the rankings of function words:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_function_fd = nltk.FreqDist(sh_function)\n",
    "sh_func_sorted = [l for r, l in sorted([(v, k) for k, v in sh_function_fd.items()], reverse = True)]\n",
    "print(sh_func_sorted[:10], end = '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_function_fd = nltk.FreqDist(ma_function)\n",
    "ma_func_sorted = [l for r, l in sorted([(v, k) for k, v in ma_function_fd.items()], reverse = True)]\n",
    "print(ma_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_words = word_tokenizer.tokenize(ho.lower())\n",
    "ho_function = [w for w in ho_words if w in function_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho_function_fd = nltk.FreqDist(ho_function)\n",
    "ho_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho_function_fd.items()], reverse = True)]\n",
    "print(ho_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, the coefficient is higher when we compare \"Othello\" to the Shakespeare corpus; but the difference is much more noticeable here, suggesting that the ranking of function words is a much better predictor.*\n",
    "\n",
    "*We may have just been lucky, so let's repeat the experiment with Marlowe's \"The Massacre at Paris\".*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho2_file = sorted(glob.glob(os.path.join(path, \"Holdout2.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho2_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho2 = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho2 = re.sub(k, v, ho2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_pos = [p[1] for p in nltk.pos_tag(ho2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_pos_fd = nltk.FreqDist(ho2_pos)\n",
    "ho2_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho2_pos_fd.items()], reverse = True)]\n",
    "print(ho2_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_sc = spearman_correlation(ranks_from_sequence(ho2_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho2_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*When considering ranks of POS tags, the correlation is again higher for the Shakespeare corpus, which is the wrong result i this case.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho2_words = word_tokenizer.tokenize(ho2.lower())\n",
    "ho2_function = [w for w in ho2_words if w in function_words]\n",
    "ho2_function_fd = nltk.FreqDist(ho2_function)\n",
    "ho2_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho2_function_fd.items()], reverse = True)]\n",
    "print(ho2_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho2_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho2_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*However, once again the rankings of function words is correct, and by a comparatively large margin.*\n",
    "\n",
    "*Let's try it a third time, with \"Romeo & Juliet\":*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\mjcor\\\\Desktop\\\\ProgrammingStuff\\\\nltk\\\\PGtexts\"\n",
    "ho3_file = sorted(glob.glob(os.path.join(path, \"Holdout3.txt\")))\n",
    "texts = []\n",
    "\n",
    "\n",
    "for fn in ho3_file:\n",
    "    with open(fn, encoding = 'utf8') as f:\n",
    "        texts.append(f.read())\n",
    "ho3 = ' '.join(texts)\n",
    "\n",
    "rep = {r'\\n': ' ', r\"\\'\": \"'\", r'\\ufeff': ' '}\n",
    "\n",
    "for k, v in rep.items():\n",
    "    ho3 = re.sub(k, v, ho3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_pos = [p[1] for p in nltk.pos_tag(ho3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_pos_fd = nltk.FreqDist(ho3_pos)\n",
    "ho3_tags_sorted = [l for r, l in sorted([(v, k) for k, v in ho3_pos_fd.items()], reverse = True)]\n",
    "print(ho3_tags_sorted, end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_sc = spearman_correlation(ranks_from_sequence(ho3_tags_sorted), \n",
    "                               ranks_from_sequence(sh_tags_sorted))\n",
    "\n",
    "sh_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ma_sc = spearman_correlation(ranks_from_sequence(ho3_tags_sorted), \n",
    "                               ranks_from_sequence(ma_tags_sorted))\n",
    "\n",
    "ma_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Again, the wrong answer for the POS-tag ranking correlation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ho3_words = word_tokenizer.tokenize(ho3.lower())\n",
    "ho3_function = [w for w in ho3_words if w in function_words]\n",
    "ho3_function_fd = nltk.FreqDist(ho3_function)\n",
    "ho3_func_sorted = [l for r, l in sorted([(v, k) for k, v in ho3_function_fd.items()], reverse = True)]\n",
    "print(ho3_func_sorted[:10], end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shf_sc = spearman_correlation(ranks_from_sequence(ho3_func_sorted), \n",
    "                               ranks_from_sequence(sh_func_sorted))\n",
    "\n",
    "shf_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_sc = spearman_correlation(ranks_from_sequence(ho3_func_sorted), \n",
    "                               ranks_from_sequence(ma_func_sorted))\n",
    "\n",
    "maf_sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But comparing the ranking of function words again indicates the correct author, and by a decent-sized margin.*\\\n",
    "\n",
    "*This sample was quite small, but it does confirm some of the results of Zhao & Zobel - namely, the frequency of use of function words is a fairly reliable indicator of authorship.  Also interesting was the fact that fairly primitive methods were more accurate than complex machine learning predictors.  Sometimes, the best model might indeed be the simplest one.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 28. \n",
    "\n",
    "★ Study gender-specific lexical choice, and see if you can reproduce some of the results of ~~http://www.clintoneast.com/articles/words.php~~.\n",
    "\n",
    "*The link is dead (Long live the link!).  Since the author's of the NLTK book neglected to include any other information about this study, it's going to be impossible to try to locate it at another site.*\n",
    "\n",
    "##### 29. \n",
    "\n",
    "★ Write a recursive function that pretty prints a trie in alphabetically sorted order, e.g.:\n",
    "\n",
    "```\n",
    "chair: 'flesh'\n",
    "---t: 'cat'\n",
    "--ic: 'stylish'\n",
    "---en: 'dog'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'c': {'h': {'a': {'t': {'value': 'cat'}, 'i': {'r': {'value': 'flesh'}}},\n",
       "   'i': {'e': {'n': {'value': 'dog'}}, 'c': {'value': 'stylish'}}}}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trie = {}\n",
    "insert(trie, 'chat', 'cat')\n",
    "insert(trie, 'chien', 'dog')\n",
    "insert(trie, 'chair', 'flesh')\n",
    "insert(trie, 'chic', 'stylish')\n",
    "trie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This uses a recursive function to retrieve values, and a separate function to pretty print them. It is difficult - if not impossible - to do this with a single recursive function.  For one thing, a trie is a dictionary, and dictionaries are not listed alphabetically.  So, we would need to retrieve all the values from the trie and sort them alphabetically before we begin deleting duplicate characters.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original - do not modify\n",
    "\n",
    "def trawl_trie(trie):\n",
    "    unsorted = []\n",
    "    for key, value in trie.items():\n",
    "        if 'value' not in key:\n",
    "            for item in trawl_trie(trie[key]):\n",
    "                unsorted.append(key + item)\n",
    "        else:\n",
    "            unsorted.append(': ' + value)\n",
    "            \n",
    "    return unsorted\n",
    "            \n",
    "def print_trie(trie):\n",
    "    \n",
    "    alphabetized = list(sorted(set(trawl_trie(trie))))\n",
    "    \n",
    "    print(alphabetized[0])\n",
    "    for k in range(1, len(alphabetized)):\n",
    "        prev_w, prev_d = (re.findall(r'(\\w+):', alphabetized[k - 1]), re.findall(r': (\\w+)', alphabetized[k - 1]))\n",
    "        curr_w, curr_d = (re.findall(r'(\\w+):', alphabetized[k]), re.findall(r': (\\w+)', alphabetized[k]))\n",
    "        word = ''\n",
    "        for i in range(min(len(prev_w[0]), len(curr_w[0]))):\n",
    "            if prev_w[0][i] == curr_w[0][i]:\n",
    "                word += prev_w[0][i]\n",
    "\n",
    "        curr_w[0] = re.sub(word, '-' * len(word), curr_w[0])\n",
    "        print(curr_w[0] + \": \" + str(curr_d[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair: flesh\n",
      "---t: cat\n",
      "--ic: stylish\n",
      "---en: dog\n"
     ]
    }
   ],
   "source": [
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Can't get anything else to work...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trawl_trie(trie):\n",
    "    unsorted = []\n",
    "    for key, value in trie.items():\n",
    "        print(unsorted)\n",
    "        if 'value' not in key:\n",
    "            for item in trawl_trie(trie[key]):\n",
    "                unsorted.append(key + item)\n",
    "                \n",
    "        else:\n",
    "            unsorted.append(': ' + value)\n",
    "            \n",
    "            \n",
    "    return list(sorted(set(unsorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['t: cat']\n",
      "[]\n",
      "[]\n",
      "['air: flesh', 'at: cat']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['en: dog']\n",
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['chair: flesh', 'chat: cat', 'chic: stylish', 'chien: dog']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trawl_trie(trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for k in trawl_trie(trie):\n",
    "    print('chien' in k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(['chien' in k for k in trawl_trie(trie)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair: flesh\n",
      "---t: cat\n",
      "--ic: stylish\n",
      "---en: dog\n"
     ]
    }
   ],
   "source": [
    "print_trie(trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trawl_trie(trie):\n",
    "    unsorted = []\n",
    "    for key, value in trie.items():\n",
    "        if 'value' not in key:\n",
    "            if sum([key in u for u in unsorted]) == 0:\n",
    "                print(key, end = '')\n",
    "            else:\n",
    "                print('-', end = '')\n",
    "            for item in trawl_trie(trie[key]):\n",
    "                unsorted.append(item)\n",
    "        else:\n",
    "            print(\": \", end = '')\n",
    "            print(value)\n",
    "           \n",
    "            \n",
    "    return list(sorted(set(unsorted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat: cat\n",
      "ir: flesh\n",
      "ien: dog\n",
      "c: stylish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trawl_trie(trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trawl_trie(trie):\n",
    "    unsorted = []\n",
    "    for key, value in trie.items():\n",
    "        if 'value' not in key:\n",
    "            for item in trawl_trie(trie[key]):\n",
    "                if len(unsorted) > 1:\n",
    "                    for u in unsorted:\n",
    "                        kl = len(key) - 1\n",
    "                        if u[kl] == key:\n",
    "                            print(\"-\", end = '')\n",
    "                        else:\n",
    "                            print(key, end = '')\n",
    "                else:\n",
    "                    print(key, end = '')\n",
    "                \n",
    "                unsorted.append(key + item)\n",
    "        else:\n",
    "            unsorted.append(': ' + value)\n",
    "            \n",
    "        unsorted = list(sorted(set(unsorted)))\n",
    "        \n",
    "            \n",
    "            \n",
    "    return unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chair: flesh', 'chat: cat', 'chic: stylish', 'chien: dog']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trawl_trie(trie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = trawl_trie(trie)\n",
    "'chat' in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabetized = list(sorted(set(list_words(trie))))\n",
    "\n",
    "for i in range(len(alphabetized)):\n",
    "    if i == 0:\n",
    "        print(alphabetized[i])\n",
    "    else:\n",
    "        flag = True\n",
    "        for j in range(len(alphabetized[i])):\n",
    "            if flag:\n",
    "                if alphabetized[i - 1][j] == ':' or alphabetized[i][j] == ':':\n",
    "                    flag = False\n",
    "                    print(':', end = '')\n",
    "                elif alphabetized[i - 1][j] == alphabetized[i][j]:\n",
    "                    print('-', end = '')\n",
    "                else:\n",
    "                    flag = False\n",
    "                    print(alphabetized[i][j], end = '')\n",
    "            else:\n",
    "                print(alphabetized[i][j], end = '')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = ['chair: flesh', 'chat: cat', 'chic: stylish', 'chien: dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unsorted[0])\n",
    "for k in range(1, len(unsorted)):\n",
    "    prev_w, prev_d = (re.findall(r'(\\w+):', unsorted[k - 1]), re.findall(r': (\\w+)', unsorted[k - 1]))\n",
    "    curr_w, curr_d = (re.findall(r'(\\w+):', unsorted[k]), re.findall(r': (\\w+)', unsorted[k]))\n",
    "    word = ''\n",
    "    for i in range(min(len(prev_w[0]), len(curr_w[0]))):\n",
    "        if prev_w[0][i] == curr_w[0][i]:\n",
    "            word += prev_w[0][i]\n",
    "            \n",
    "    curr_w[0] = re.sub(word, '-' * len(word), curr_w[0])\n",
    "            \n",
    "    \n",
    "    print(curr_w[0] + \": \" + str(curr_d[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_differing characters(word1, word2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 'word1'\n",
    "w2 = 'worter2'\n",
    "\n",
    "word = ''\n",
    "for i in range(min(len(w1), len(w2))):\n",
    "    if w1[i] == w2[i]:\n",
    "        word += w1[i]\n",
    "    \n",
    "w2 = re.sub(word, '-' * len(word), w2)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'---ter2'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
